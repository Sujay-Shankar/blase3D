\documentclass[twocolumn]{aastex631}
\usepackage{hyperref}
\bibliographystyle{aasjournal}
%\turnoffedit
\usepackage[caption=false]{subfig}
\usepackage{booktabs}
\usepackage{censor}
\usepackage[arrowdel]{physics}
\usepackage{mathtools}
\usepackage{outlines}
\usepackage{lipsum}  
\usepackage{amsmath,bm}

\definecolor{belize}{RGB}{41, 128, 185}
\definecolor{peter}{RGB}{52, 152, 219}
\definecolor{nephritis}{RGB}{39, 174, 96}
\definecolor{asbestos}{RGB}{127, 140, 141}
\definecolor{clouds}{RGB}{236, 240, 241}

\hypersetup{linkcolor=belize, citecolor=belize,filecolor=asbestos,urlcolor=peter}

%\setlipsum{
%  par-before = \begingroup\color{clouds},
%  par-after = \endgroup
%}
%\SetLipsumParListSurrounders{\begingroup\color{gray}}{\endgroup}

\hyphenation{long-it-udin-ally hy-po-thet-ical de-mon-stra-tion Ad-min-is-tra-tion}

\DeclareMathOperator{\resample}{resample}

\def\Teff{T_{\rm eff}}
\def\vsini{v\sin{i}}
\def\kmps{\mathrm{km}\;\mathrm{s}^{-1}}

\begin{document}
\shorttitle{Blas\'e}
\shortauthors{Gully \& Morley}
\title{Transfer learning for \'echelle spectroscopy with \texttt{blas\'e}}

\author{Michael Gully-Santiago}
\affiliation{The University of Texas at Austin Department of Astronomy}

\author{Caroline V. Morley}
\affiliation{The University of Texas at Austin Department of Astronomy}

\begin{abstract}

    Comparison of \'echelle spectra to synthetic models has become a computational statistics challenge, with over ten thousand individual spectral lines affecting a typical cool star \'echelle spectrum.  Telluric artifacts, imperfect line lists, inexact continuum placement, and inflexible models frustrate the scientific promise of these information-rich datasets.  Here we debut an interpretable machine-learning framework \emph{blas\'e} that addresses these and other challenges.  The semi-empirical approach can be viewed as ``transfer learning''--first pre-training models on noise-free precomputed synthetic spectral models, then learning the corrections to line depths and widths from whole-spectrum fitting to an observed spectrum.  The auto-differentiable model employs back-propagation, the fundamental algorithm empowering modern Deep Learning and Neural Networks. Here, however, the 40,000+ parameters symbolize physically interpretable line profile properties such as amplitude, width, location, and shape, plus radial velocity and rotational broadening.  This hybrid data-/model- driven framework allows joint modeling of stellar and telluric lines simultaneously, a potentially transformative step forwards for mitigating the deleterious telluric contamination in the near-infrared.  The \emph{blas\'e} approach also acts as a deconvolution tool. It is suitable for Doppler Imaging scenarios with longitudinally symmetric surface features like bands or polar spots, which evade detection in differential techniques.  Blas\'e can also create super-resolution semi-empirical templates useful for critically evaluating atomic and molecular line lists.  Its sparse-matrix architecture and GPU-acceleration make \emph{blas\'e} fast, with end-to-end training of an octave of bandwidth (50+ \'echelle orders) in under 1 minute.  The open-source PyTorch-based code \texttt{blase} includes tutorials, deep dives, reproducible Python environments, and API documentation.  We show how the tool fits into the existing Python spectroscopy ecosystem, demonstrate a range of astrophysical applications, and discuss limitations and future extensions.


\end{abstract}

\keywords{High resolution spectroscopy (2096), Stellar spectral lines (1630), Astronomy data modeling(1859), GPU Computing (1969), Calibration (2179), Radial Velocity (1332), Maximum likelihood estimation (1901), Deconvolution (1910), Atomic spectroscopy (2099), Stellar photospheres (1237)}

\section{Introduction}\label{sec:intro}

\subsection{Spectral fitting past and present}

Tens of thousands or more individual spectral lines give rise to a sea of undulations that imbue each stellar spectrum with its characteristic appearance.  The identification and understanding of these lines have defined a large category of astrophysics over the last century.  The field grew from by-eye catalogs of stellar templates \citep{1901AnHar..28..129C} to quantifying the role of atomic ionization balance \citep{1925PhDT.........1P}, to modern synthetic forward models including millions or billions of lines \citep[\emph{e.g.}][]{husser13, 2021ApJ...920...85M}.  As technology has improved, our data and models have become more voluminous, more precise, and more complicated.  The mere act of comparing models to observed spectra can now resemble a computational statistics challenge as much as a scientific one.  Here we introduce a new machine-learning-based framework \texttt{blas\'e} aimed at solving computational, statistical, and scientific challenges associated with data-model comparisons for modern astronomical spectroscopy.

The metaphorical holy grail of astronomical spectroscopy is a function that takes in an observed stellar spectrum and reports back the position, amplitude, width, and shape of all of its spectral lines, automatically, accurately, and precisely.  The function would go further. It would report back the systemic radial velocity ($RV$) and rotational broadening ($v\sin{i}$)and optionally fundamental stellar properties, including $T_{\mathrm{eff}}$, $\log{g}$, and $[\mathrm{Fe}/\mathrm{H}]$.  Finally, the function would---in a feat of artificial intelligence---provide what it believes to be the interpretable generating function that produced this data in the first place, so that we may gain insights on future examples of this or other stars.  Solving this problem is hard, for at least four reasons.  First, the spectral lines may overlap, and so the assignment of one line may be partially degenerate with the assignment of some other adjacent line.  Second, extremely wide line wings blend into the continuum, such that the placement of the continuum level may become ill-defined.  Third, the extent of line blending and realized line shape depends strongly on the spectral resolution of the spectrograph, the rotational broadening of the star, and possibly the instrumental configuration at the time of observation.  Finally, telluric absorption lines commingle with the astronomical spectral lines of interest, censoring some spectral regions entirely, or partially confounding other lines with chance alignments.

Addressing these and other challenges forms the backbone of \emph{spectral calibration}, an increasingly valuable specialty as the deficits in our models become intolerable with greater data quantity and quality.  Luckily, many scientific applications in astrophysics do not need the technically demanding noise-free template, nor catalog-of-all-spectral-lines.  A few lines suffice.  For those applications, human inspection of isolated lines and semi-automated equivalent width determination have been and will remain adequate.

But many new and important questions in the fields of stars and exoplanets aspire to reach the margins of what the entire dataset can inform.  In particular, data from high-\emph{grasp} \'echelle spectrographs possess simultaneously high spectral resolving power and high bandwidth, yielding tens of thousands or possibly millions of independent spectral resolution elements for each star, substar, or exoplanet.  Those applications that seek to gain signal by ``stacking'' spectral lines or cross-correlating with templates can hypothetically gain huge boosts in the accessible signal-to-noise ratio compared to a single or few lines.  Most manual and semi-automated methods cannot take advantage of the entire spectral bandwidth, or rely on exact knowledge of the underlying templates and may fail to achieve the hypothetical promise of these high-bandwidth spectrographs \citep{2020AJ....160..198H}.

For example, exoplanet cross-correlation spectroscopy \citep{2013MNRAS.436L..35B} hinges on accurate molecular spectral templates to detect and characterize the atmospheres of exoplanets.  Imperfections in these templates can mute the perceived signal strength of these atmospheric features \citep{2015A&A...575A..20H}.

In extreme precision radial velocity (EPRV) applications, cross-correlation methods work \citep{2018A_A...620A..47D}, but have many limitations \citep{2022arXiv220110639Z}.  Among the many such limitations, one pernicious noise floor stands out as enigmatic: telluric mitigation.  Many practitioners today simply mask these telluric regions, yet micro-tellurics still inject variance into the spectrum that cannot be easily accounted for with existing methods.  Instead, a robust accounting of telluric absorption at the $\mathrm{cm/s}$ level may require joint modeling of the star and the Earth's atmospheric absorption \emph{before} convolution with an instrumental kernel.  This telluric joint modeling capability does not yet exist at a precision that can meet these strenuous demands.

In the case of Doppler imaging, an accurate underlying spectral template is needed to detect longitudinally symmetric structures \citep{1983PASP...95..565V,2021arXiv211006271L} such as polar spots \citep{roettenbacher16} or zonal bands \citep{Crossfield14,2021ApJ...906...64A}.  There exists a nearly circular reasoning: we need to know the extent of line profile perturbations to reveal the underlying spectral template, but we need the underlying spectral template to estimate the line profile perturbations.  The approach introduced here offers a path forward.

\subsection{Automatic differentiation technology}

Existing open-source frameworks have overcome some of these challenges, or have been purpose-built for specialized applications.
These frameworks include \texttt{ROBOSPECT} \citep{2013PASP..125.1164W}, \texttt{specmatch} \citep{2015PhDT........82P}, \texttt{specmatch-emp} \citep{2017ApJ...836...77Y}, \texttt{wobble} \citep{2019AJ....158..164B}, \texttt{starfish} \citep{czekala15}, \texttt{sick} \citep{2016ApJS..223....8C}, \texttt{psoap} \citep{2017ApJ...840...49C}, \texttt{FAL} (Cargile et al. \emph{in prep}), CHIMERA \citep{2015ApJ...807..183L}, the \texttt{Cannon} \citep{2017ApJ...836....5H},  \texttt{MOOG} \citep{2012ascl.soft02009S}, \texttt{MOOGStokes} \citep{2013AJ....146...51D}, \texttt{MINESweeper} \citep{2020ApJ...900...28C}, and recently \texttt{ExoJAX} \citep{2022ApJS..258...31K}.
The designs of these frameworks necessarily have to make a choice in the bias-variance tradeoff: is the tool more \emph{data}-driven or more \emph{model}-driven?  The statistical tradeoff can be viewed as a concession in physical self-consistency for model flexibility: more or fewer parameters; more \emph{accurate} or more \emph{precise}.

A key new enabling technology breaks these classical tradeoffs in data-model comparisons for astronomical spectroscopy.  Automatic differentiation \citep[``autodiff'' or ``autograd'',][]{2015arXiv150205767G, 2016PhDT.......317M} and its affiliated backpropagation algorithm \citep{kelley1960,Linnainmaa1976TaylorEO, 1986Natur.323..533R, 1990JGCD...13..926D} has revolutionized machine learning and neural network architecture design, and is increasingly applied in astrophysical data analysis contexts, \emph{e.g.} kernel phase coronography with \texttt{poppy} \citep{2021ApJ...907...40P}, and exoplanet orbit fitting with \texttt{exoplanet} \citep{2021JOSS....6.3285F}.  Of the spectroscopy frameworks mentioned above, the \texttt{TensorFlow}-based \citep{tensorflow2015-whitepaper} \texttt{wobble} and the \texttt{JAX}-based \citep{jax2018github} \texttt{ExoJAX} employ autodiff technology.  \texttt{wobble} treats each pixel as a tunable control point, producing $\sim10^5$ parameters for a modern stellar spectrum.  The \texttt{ExoJAX} framework has only $\sim$dozens of tunable parameters that describe the fundamental physical properties controlling a brown dwarf atmosphere.  These two autodiff-aware frameworks span the extreme ends of non-parametric and parametric modeling for spectroscopy.

In this paper, we show that autodiff-aware semi-empirical models offer an appealing middle ground: informed by self-consistent models but refined with data.  This sweet spot in the bias-variance tradoff can be thought of as a hybrid data-and-model driven approach.  The algorithm presented here focuses on modeling the spectra of stars and brown dwarfs.  In Section \ref{methodology} we introduce the interpretable forward-model design and its \texttt{PyTorch}-based \citep{2019arXiv191201703P} implementation, \texttt{blas\'e}.  Existing stellar models \citep[\emph{e.g.}][]{husser13} and substellar models \citep[\emph{e.g.}][]{2021ApJ...920...85M} laboriously solve for a self consistent thermal structure in the atmosphere given the copious opacity sources that themselves depend on temperture and pressure.  Here we build upon that hard work by cloning pre-existing synthetic stellar or substellar models (Section \ref{methodology}), and optionally by cloning models of Earth's atmospheric ``telluric'' absorption (Section \ref{sectionTelluric}). In Section \ref{transferLearn} we describe how to adapt both stellar and telluric cloned models to real-world \'echelle data simultaneously, using a transfer-learning step.  Section \ref{secInjRec} validates the procedure from end-to-end with systematic injection-recovery tests on simulated data of known properties, quantifying the limits of the technique.  Finally, we compare the framework to alternatives (Section \ref{secDiscuss}) and chronicle many conceivable extensions for future work (Section \ref{secFutureWork}).


\section{Methodology I: Cloning stellar spectra}\label{methodology}

\subsection{Overall Architecture and Design Choices}

We start with a high resolution pre-computed synthetic stellar or substellar model spectrum, $\mathsf{S}_{\rm abs}(\bm{\lambda})$ at its native resolution sampling and with its original absolute physical flux units. The procedure that follows is largely agnostic to the exact details of how this spectrum was made, or what physics or chemistry it may represent. For the purposes of this paper, we will showcase examples from two well-known families of precomputed synthetic astronomical spectra: \texttt{PHOENIX} \citep{husser13} for stellar spectra $(T_{\mathrm{eff}}\in [2300, 10000]\;K)$ and \texttt{Sonora} \citep{2021ApJ...920...85M} for substellar spectra $(T_{\mathrm{eff}}\in [200, 2300]\;K)$. The algorithms in the framework may also work for precomputed synthetic spectra of reflected light exoplanets, supernovae, galaxies, or even further afield such as laboratory physical chemistry, plasma physics, or materials science.

We place mild demands on the precomputed spectra. They should have sporadic regions of discernable continuum devoid of lines, and the continuum should vary smoothly in wavelength. The spectral lines or pseudo-lines should be resolved (and not sub-sampled). We suspect most stellar spectra meet these criteria, except for the coolest M dwarfs and brown dwarfs. The method can handle spectra without discernable continua, with some fine-adjustments to the preprocessing steps. We truncate the red and blue limits of the precomputed synthetic spectrum to match a high-bandwidth echelle spectrograph, extended with a buffer at the edges of size $\pm \Delta \lambda_{\mathrm{buffer}}$, chosen to account for plausible radial velocity and rotational broadening of real stars. A generous buffer of $v \sin{i} < 500 \;\kmps$ and $|RV|<500 \;\kmps$ yields a typical buffer of about 30~\AA.

The choice of limiting the bandwidth to a region of interest around a single echelle spectrograph bandwidth stems from computational constraints. In principle, there is no fundamental limit to the bandwidth one could clone with the method presented here, up to and including the entire precomputed synthetic spectral model bandwidth. We adopt the exact native wavelength sampling with no smoothing or interpolation, yielding a wavelength vector $\bm{\lambda}_S$ with length $N_\mathrm{S}$ equal to the number of pixels within the extents of our region of interest including the buffers.

At this stage, we have the choice of whether to work in linear or log scale flux units. Adopting the log of the flux would ensure that the cloned model possesses only positive flux values, a desirable trait of any physical spectral model. We have implemented both modes in \texttt{blas\'e}, allowing users to choose their preference.  We only narrate the linear flux unit description in the main text of this document, for the sake of clarity, and since most practitioners may tend to think of flux in terms of linear flux units.  The data-model comparison step will always take place in linear flux units, so the only operational difference is the behavior for deep and saturated lines.  Appendix \ref{appendixLogScale} lists the equations adjusted to log flux units.

\subsection{Initialization}

We initialize the cloned model with a series of preprocessing steps. We divide the entire spectrum by a black body $\mathsf{B}(\bm{\lambda}_S)$ of the same effective temperature $T_{\mathrm{eff}}$ as the model template. The resulting signal usually still has smooth wiggles around the continuum. An optional continuum flattening step ensures that subsequent spectral line finding steps get applied uniformly. This high-pass filtering step should be set to capture the genuine spectral shape, without over-fitting broad line wings such as those in deep Hydrogen and sodium lines. Any high-pass filter will work, a Gaussian Process approach would be ideal.  Instead we apply a simple and familiar heuristic: fit a polynomial $\mathsf{P}(\bm{\lambda}_S)$ to a few continuum peaks and divide out the trend.

The result should be a flattened ``continuum-normalized'' spectrum familiar to practitioners in high-resolution spectroscopy, with the continuum level close to unity.  It is this spectrum that will serve as the centerpiece of subsequent training steps.  We, therefore, drop any subscript and simply refer to this flattened spectrum as $\mathsf{S}$:

\begin{eqnarray}
    %\mathsf{S} = \frac{\mathsf{S}_{\rm abs}}{\mathsf{B} \; \mathsf{P}}
    \mathsf{S} = \mathsf{S}_{\rm abs}/\mathsf{B} / \mathsf{P}
    \label{eqnFlattening}
\end{eqnarray}


where the division indicates element-wise division of these arrays or ``vectors'' of flux values.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/N_lines_vs_Teff_prom.png}
    %\includegraphics[width=0.45\textwidth]{figures/N_lines_vs_Teff_prom.png}
    \includegraphics[width=0.41\textwidth]{figures/line_density_demo.png}
    \caption{Scaling of prominence and density of spectral lines.
        \emph{Top:} Number of lines versus effective temperature for PHOENIX models truncated to IGRINS (\textbf{blue, connected} points) and HPF (\emph{red, free-standing} points) bandwidths, for different prominence thresholds of $0.02$, $0.01$, and $0.005$.
        \emph{Bottom:} Number of lines per 100 \AA\ wavelength bin for stellar (\emph{blue, upper envelope of steps}) and telluric (\emph{orange, lower envelope of steps}), illustrated for a $T_\mathrm{eff}=4700$~K, $\log{g}=4.5$ PHOENIX model and a $T=290 \mathrm{K}$, relative humidity of 40\% TelFit model.}
    \label{fig_Nlines_vs_teff}
\end{figure}


% TODO We pre-assign spectral lines for the conspicuous Hydrogen lines and neutral metal lines. Table

We emphasize that a recreation of the unvarnished input spectrum---if desired---can be obtained by multiplying the continuum-flattened signal by the ``perturbed black body'', $\mathsf{B}(\bm{\lambda}_S)\odot \mathsf{P}(\bm{\lambda}_S)$, that symbolizes the black body modulated by continuum opacity or broad-band radiative transfer effects. This smooth spectrum may be useful for applications that need to keep track of broad-band flux, such as low-resolution spectra, or regions with molecular band heads. The ``perturbed black body'' continuum model contains $n_{\mathrm{poly}}+1$ fixed-but-possibly tunable lookup parameters, plus the fixed input $T_{\mathrm{eff}}$.  For most practitioners these terms serve as nuisance parameters and are perfunctorily discarded.

Next, we identify the spectral lines. We apply a local-minimum-finding algorithm \citep{2020SciPy-NMeth} on the spectrum by defining a prominence threshold $P_{\rm rom} \in (0.005, 0.02)$. This threshold dictates the number of lines that will be modeled: a lower prominence finds more, weaker lines, and a larger prominence finds fewer, deeper lines. The prominence algorithm successfully finds lines that reside on top of broad line wings, or unresolved band heads provided that the individual lines exceed the prominence threshold in their local region. The number of lines $N_{\mathrm{lines}}$ depends on the bandwidth, prominence, and the intrinsic properties of the input spectrum, principally effective temperature and metallicity.

For this paper, we illustrate examples for two \'echelle spectrographs with particularly large spectral grasp: the Habitable Zone Planet Finder \citep[HPF,][]{2014SPIE.9147E..1GM} on the Hobby-Eberly Telescope at McDonald Observatory in Fort Davis, Texas; and the Immersion Grating Infrared Spectrograph \citep[IGRINS,][]{park14}
currently on the Gemini South Telescope on Cerro Pach\'on in Chile. The $R=55,000$ HPF has a native bandwidth of $8079-12785$~\AA, which we expand to $8049-12815$~\AA~including the edge buffers. IGRINS has two cameras for $H$ and $K$ band, with the combined spectrum spanning $14267-25217\;$\AA~ including the edge buffers and the region in-between the two cameras, all at a resolving power of $(R=45,000)$. The spectrograph acquisition, reduction, and post-processing steps yield data $\mathsf{D}(\bm{\lambda}_{D})$, where $\bm{\lambda}_{D}$ is the wavelength vector at the instrumental resolution and sampling of each instrument, generally much coarser than the resolution and sampling grid of the precomputed synthetic spectra. The data wavelength vector may also contain gaps between \'echelle orders, whereas the precomputed wavelength coordinates are usually contiguous. HPF may have up to $2048\times28=$57,344 pixels, and IGRINS has typically about 75,000 pixels, after common trimming of noisy edge pixels and unusable telluric regions.





Figure \ref{fig_Nlines_vs_teff} shows how the number of detected lines $N_{\mathrm{lines}}$ scales with effective temperature and prominence threshold $P_{\rm rom}$ for the \texttt{PHOENIX} grid, truncated to the bandwidths-plus-buffers for HPF and IGRINS. We see between about 2,000 and 20,000 lines depending on the $T_{\mathrm{eff}}$ and $P_{\rm rom}$. HPF and IGRINS have a comparable number of lines, and halving the prominence increases the number of lines by about $20-30\%$ in these ranges. The number of lines monotonically increases towards cooler effective temperatures.
The HPF-truncated spectra have $N_s=$335,849 native resolution samples, comparable to the IGRINS-truncated spectra, $N_s=$330,052.

So far we have only one piece of information about the peaks: their location. Next, we derive coarse properties about each detected peak: its amplitude and width, again using the prominence algorithms implemented in \texttt{scipy} \citep{2020SciPy-NMeth}.

There does not exist a general-purpose, single-shot algorithm for obtaining the lineshape in the presence of overlapping spectral lines: where do the wings of one line begin and the wings of another adjacent line end? We, therefore, do not attempt to determine anything about the lineshape at this stage and instead assume that the lines resemble a Voigt profile, with a guess width about equally split between Lorentzian and Gaussian.

\subsection{The \texttt{blas\'e} Stellar Clone Model}

We have now arrived at the \texttt{blas\'e} clone model $\mathsf{S}_{\rm clone}(\bm{\lambda}_S)$ for a flattened synthetic spectrum $\mathsf{S}$: it is the cumulative product of transmission through the sea of all overlapping spectral lines:

\begin{eqnarray}
    \mathsf{S}_{\rm clone} = {\displaystyle \prod_{j=1}^{N_{\mathrm{lines}}} 1-a_j \mathsf{V}_j } \label{equation1}
\end{eqnarray}

where $\mathsf{V}_j$ is the Voigt profile $\mathsf{V}(\bm{\lambda}_S-\lambda_{\mathrm{c},j}, \sigma_j, \gamma_j)$ with Gaussian standard deviation $\sigma$, Lorentzian half-width $\gamma$, at line center position $\lambda_c$, for the $j^{th}$ spectral line. The amplitude $a$ is always expected to be positive for absorption lines.

The Voigt profile $\mathsf{V}(\lambda, \sigma_j, \gamma_j)$ can be computed in exact closed-form using the Voigt-Hjerting function \citep{1938ApJ....88..508H} as the real part of the complex Fadeeva function \citep[\emph{e.g.}][]{2011arXiv1106.0151Z}. Evaluation of the Fadeeva function can be computationally costly, and so approximate forms may be desirable. Here we adopt the pseudo-Voigt approximation \citep{Ida:nt0146}.


\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/blase_flowchart03.pdf}
    \caption{Visual flowchart of the \texttt{blas\'e} forward model.  \textbf{Step 0} (not shown) is to choose a precomputed synthetic stellar spectrum---and, optionally, a precomputed synthetic telluric spectrum---with physical properties close to the target and observing conditions.  Both the stellar and telluric spectra get cloned (\textbf{Step 1}).  The stellar model is warped to its extrinsic properties (\textbf{Step 2}), and then the stellar and telluric models get multiplied together (\textbf{Step 3}).  Next, this joint model is convolved with an instrumental kernel and resampled to the wavelength coordinates of the data spectrum (\textbf{Step 4}).  It is this forward model that gets directly compared to the observed spectrum (not shown).
    }
    \label{blase_flowchart}
\end{figure*}

\subsection{Goodness of fit metric}
The model evaluated with its coarse initial values would have terrible performance: it would only vaguely resemble the synthetic spectral model, with up to $\pm 50\%$ undulations from the inexact assignment of widths, lineshapes, and amplitudes. Instead, we tune the parameters of the model, starting from these coarse initial values. This model has between $N_{\mathrm{lines}}\times 1$ and $N_{\mathrm{lines}}\times 4$ free parameters, depending on how many of the 4 physical line properties you wish to tune.  We default to fitting three parameters per line, where the center wavelength is held fixed and the amplitude, width, and lineshape are allowed to vary. We minimize a scalar ``goodness-of-fit'' metric, \emph{aka} loss scalar $\mathcal{L}$, chosen as the mean squared error (MSE), which is proportional to $\chi^2$, the sum of the squares of the residual vector $\mathsf{R} \equiv \mathsf{S}-\mathsf{S}_{\rm clone}$ but has no notion of per-pixel noise since the precomputed synthetic spectrum has no uncertainty:

\begin{eqnarray}
    \mathcal{L} = \sum_i^{N_S} (S_i - S_{\mathrm{clone},i})^2 = \mathsf{R^\intercal}\cdot \mathsf{R} \label{simpleLikelihood}
\end{eqnarray}


As seen in Figure \ref{fig_Nlines_vs_teff}, the number of lines can exceed 7,000, meaning the clone model has over 21,000 free parameters. Fitting more than about 300 parameters is difficult with conventional optimizers that rely on minmizing a single scalar loss: the search space becomes too large.  Here we employ a variant of Stochastic Gradient Descent (SGD), a transformative optimization technique that can scale to a virtually unlimited number of parameters \citep{2016arXiv160904747R}. This technique computes the derivative of the loss scalar with respect to each of the parameters, the so-called \emph{Jacobian}: $\grad \mathcal{L} = (\frac{\partial \mathcal{L}}{\partial a_j}, \frac{\partial \mathcal{L}}{\partial \sigma_j}, \cdots, \frac{\partial \mathcal{L}}{\partial \gamma_j})$. The Jacobian indicates how the MSE would decrease with a change in the parameter-of-interest, or put simply ``which way and by how much'' you have to change each individual line property to get a better fit.

The optimizer updates the $a_j, \sigma_j, \gamma_j$ parameters by a small fraction of the Jacobian---called the learning rate LR---towards the direction that would improve the fit, for all parameters simultaneously. The Jacobian is calculated behind the scenes with automatic differentiation implemented as the so-called backpropagation algorithm or simply ``backprop'' \citep{2015arXiv150205767G}. We choose the \texttt{PyTorch} framework that computes these Jacobians efficiently for all of the mathematical primitives in our \texttt{blas\'e} implementation \citep{2019arXiv191201703P}.

It is this ability to automatically compute Jacobians that sets \texttt{PyTorch} (and \texttt{JAX} and \texttt{TensorFlow}) apart.  These frameworks give exact Jacobians, instantaneously, for free (or cheap).  Without exact Jacobians a conventional optimization step only obtains \emph{one} piece of information: how much the overall loss changed.  With exact Jacobians, we obtain $N_\mathrm{params}$ pieces of information for each evaluation of the forward model.  The power of gradient descent becomes transformative as the number of parameters grows into the tens of thousands, as is the case for \emph{blas\'e}. 

\subsection{GPU and Autodiff specific considerations}

Forward modeling with tens of thousands of physics-informed parameters may seem like such a significant paradigm shift that it can feel too good to be true.  In this section we introduce the non-negligible architectural design tradeoffs that arise when adopting an autodiff framework---such as \texttt{PyTorch}---for physics based forward modeling.  

First, we make a few tweaks to the implementation for numerical purposes. We enforce that all Gaussian and Lorenzian widths are positive by tuning the natural log of the widths, and exponentiate them before inclusion in Equation \ref{equation1}. Based on the minus sign in Equation \ref{equation1}, an amplitude $a_j$ could hypothetically take on either positive values (flux loss, absorption) or negative values (flux gain, emission).  For now we focus on photospheric absorption lines---as opposed to, say, chromospheric emission lines---and therefore enforce all the amplitudes to be positive by tuning $\ln{a_j}$ and then exponentiating in the same strategy as above.  Emission lines---if desired---could be included with a mere sign flip to Equation \ref{equation1}.  Lines that can manifest either in absorption or emission could hypothetically relax the natural log pre-processing step for isolated lines.  We narate only the absorption scenario moving forward.

The autodiff machinery has a convenient way to set which parameters are held fixed and which are iteratively fine-tuned.  One simply disables the autodiff flag for the fixed parameters: we set the \texttt{requires\_grad=True} property for any \texttt{PyTorch} tensor that we want to vary. This design allows us to easily explore whether, say, allowing the $\lambda_\mathrm{c}$ parameter to vary significantly improves the fit.

The computational bottleneck occurs at the evaluation of Equation \ref{equation1}, which can be viewed as having a $N_{\mathrm{lines}}\times N_{S}$ matrix $\bm{\bar{F}}$ assembled by stacking each Voigt absoption profile $\mathsf{V}_j(\bm{\lambda}_s)$ on top of each other:

\begin{equation}
    \begin{pmatrix}
        1 - a_1 \mathsf{V}_1(\bm{\lambda}_s)                                       & \\
        1 - a_2 \mathsf{V}_2(\bm{\lambda}_s)                                       & \\
        \vdots                                                                     & \\
        1 - a_{N_{\mathrm{lines}}} \mathsf{V}_{N_{\mathrm{lines}}}(\bm{\lambda}_s) &
    \end{pmatrix}
\end{equation}

An element of this matrix, $F_{ji}$, will have the flux value for a given $j^{th}$ line at a given $i^{th}$ wavelength coordinate. Equation \ref{equation1} performs a type of matrix contraction, turning a $N_{\mathrm{lines}}\times N_{S}$ matrix into a length $N_{S}$ row vector. The number of Floating Point Operations (FLOPS) scales with the number of entries in this matrix. So we face a tradeoff of wanting to make the matrix large for accuracy and small for computational expedience.

We can rewrite Equation \ref{equation1} as a sum by taking the log of both sides and dropping in this $\bm{\bar{F}}$ matrix:

\begin{eqnarray}
    \ln{\mathsf{S}_{\rm clone}} = \sum_{j=1}^{N_{lines}} \ln{F_{ji}} = \mathbf{1} \cdot \ln{\bm{\bar{F}}}  \label{eqnFbar}
\end{eqnarray}


where $\mathbf{1}$ is a $1\times N_{\rm lines}$ row vector of all-ones. We re\"emphasize that---in its current form---each spectral line has to be painstakingly evaluated across the entire spectral bandwidth.   Efficient GPU algorithms exist for voluminous matrix manipulations such as this one, so this voluminous computation will proceed as quickly as possible on modern machines. In particular, the proprietary CUDA architecture for NVIDIA\textsuperscript{\tiny\textregistered} GPUs contains Tensor cores with specialized matrix math. The chief bottleneck occurs when the storage of the $\bm{\bar{F}}$ matrix exceeds the available RAM of a GPU or CPU: the computation will fail with an ``Out of Memory'' exception. Modern NVIDIA GPUs have $8-40$ GB of RAM, which translates roughly to a few thousand spectral lines across $\sim$300,000 pixels.  It is generally not possible to construct Equation \ref{eqnFbar} in its entirety in one-fell-swoop, even on a GPU. A remedy is needed.


\subsection{Sparsity}

The $\ln{\bm{\bar{F}}}$ matrix is sparsely populated: most of the entries far from the line center are vanishingly close to zero. Here we take advantage of that mostly empty matrix using the mathematics of sparse matrices \citep{saad03:IMS}.

We retain a relatively small number of pixels $N_{\rm cut}$ adjacent to the line center. Setting this wing cut produces a speedup by a factor of $\frac{N_S}{N_{\mathrm{cut}}}$, which can exceed $100\times$ for wide bandwidth spectra. The choice of $N_{\rm cut}$ is nuanced.  It should be set large enough that truncation effects are not seen for the broadest lines.  But even more, $N_{\rm cut}$ has to be future-proofed for Doppler-shifting. Extreme Doppler shifts could hypothetically send line cores entirely outside the extents of $N_{\rm cut}$ if set too low.  We therefore typically set wingcuts comparable to the buffer size $2 \Delta \lambda_{\mathrm{buffer}}$, even though most weak lines only perceptibly affect $<1\;$\AA. We coerce all wing cuts to be the same number of pixels, typically 6000 pixels, $\sim30-60~$\AA~ for \texttt{PHOENIX}, with the middle pixel being at the line center position, and about 3000 pixels to the red and blue side of the line. We populate a new approximate sparse matrix $\ln{\bm{\hat{F}}}$ with only these $6000$ pixels per line and assume zeros everywhere else.

The remapping of the sparse matrix can be pictured as having shifted all lines to the center of this new matrix $\bm{\hat{F}}$, visualized pictorially in Equation \ref{eqnPictograph}.  The algorithmic machinery keeps track of each $(i, j, F_{ji})$ trio of coordinates and flux values.


\begin{equation} \label{eqnPictograph}
    \begin{aligned}
        \bm{\bar{F}} & =
        \begin{pmatrix}
            \cdots \includegraphics[height=1cm]{figures/voigt_01.pdf}  \cdots & \\
            \cdots \includegraphics[height=1cm]{figures/voigt_02.pdf}  \cdots & \\
            \vdots                                                            & \\
            \cdots \includegraphics[height=1cm]{figures/voigt_03.pdf} \cdots  &
        \end{pmatrix}                        \\
        \bm{\bar{F}} & \mapsto \bm{\hat{F}} =\begin{pmatrix}
                                                 \includegraphics[height=1cm]{figures/voigt_01b.pdf} & \\
                                                 \includegraphics[height=1cm]{figures/voigt_02b.pdf} & \\
                                                 \vdots                                              & \\
                                                 \includegraphics[height=1cm]{figures/voigt_03b.pdf} &
                                             \end{pmatrix}
    \end{aligned}
\end{equation}


Sparse matrix methods generally support an operation known as \emph{coalescing}, which sums values with repeated indices. Each pixel may get computed about $\sim100$ times in this sparse implementation, which is about $70\times$ faster than each pixel getting computed $N_\mathrm{lines}\sim7000$ times in the dense approach.  Efficient algorithms for assembling and coalescing sparse matrices exist in \texttt{PyTorch}.  Some GPUs now support additional hardware-acceleration of sparse matrices, providing even greater speedups.



\subsection{Optimization and training}

We use the Adam optimizer \citep{2014arXiv1412.6980K} with a typical learning rate $LR\in (0.005, 0.1)$ and all the defaults for \texttt{PyTorch v1.11}.  We defined the number of training epochs $N_{epoch}=100-10,000$ depending on the application. The user can optionally monitor a live view of the training progress with Tensorboard \citep{tensorflow2015-whitepaper} to gain an intuition for the training efficiency.


\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/blase_clone_demo.png}
    \caption{PHOENIX spectrum cloned with blas\'e.  This $T_{\mathrm{eff}}=4700\;$K,  $\log{g}=4.5$ solar metallicity model has 9,028 individual cloned spectral lines, each with 3 tuned parameters.  The pictured 50 \AA~ chunk contains 121 spectral lines and represents about 1$\%$ of the entire spectral bandwidth that was cloned.  Some flaws can be seen near the cores of deep lines, or wings of broad lines.}
    \label{fig_cloned_spectrum_demo}
\end{figure*}

Figure \ref{fig_cloned_spectrum_demo} shows a portion of a PHOENIX spectrum cloned with \texttt{blas\'e}. The $1000$ epochs of training took 56 seconds on an NVIDIA\textsuperscript{\tiny\textregistered} RTX2070 GPU with \texttt{PyTorch v1.11}, \texttt{CUDA v11.1}, and Intel\textsuperscript{\tiny\textregistered} Core\textsuperscript{\tiny TM} i7-9750H CPUs at 2.60GHz, with all tensors as FP64. The same computation on a 2020 M1 Macbook Air took 1$^h$25$^m$ with \texttt{PyTorch v1.9}, $90\times$ slower than the GPU counterpart.

We store the model parameters to disk and refer to the entire collection of parameters as a \emph{pre-trained model}.  More specifically this fine-tuned model represents an evaluable and interpretable clone of the original static pixel-by-pixel flux values.

\section{Methodology II: Cloning Telluric Spectra} \label{sectionTelluric}
Ground-based near-IR \'echelle spectra possess thousands of depressions attributable to molecular line absorption in Earth's atmosphere.  These telluric lines hamper the unbiased interpretation of \'echelle spectra, so some treatment plan is needed.  Often the regions of known, deep tellurics are simply discarded.  In other cases, the lines are modeled with first principles line-by-line radiative transfer \citep[\emph{e.g.} \texttt{TelFit,}][]{2014AJ....148...53G} or through data-driven means \citep[\emph{e.g.} \texttt{wobble,}][]{2019AJ....158..164B}.  The most demanding EPRV applications require a precision characterization of telluric lines that the astronomical community has not yet been able to achieve, and that may rival even the abilities of Earth Science practitioners.  A hybrid data-/model- driven approach was among the chief recommendations of the \emph{Telluric Hack Week} Workshop\footnote{\url{https://speakerdeck.com/dwhgg/telluric-line-hack-week-wrap-up}} aimed at improving mitigation of the atmosphere's deleterious effects (David W. Hogg, \emph{priv. comm.}).  The \texttt{blas\'e} framework achieves a key milestone by introducing a \emph{hybrid} approach to tellurics.

\subsection{The \texttt{blas\'e} Telluric Clone Model}
We start with a precomputed synthetic telluric model, $\mathsf{T}$ with associated wavelength coordinates $\bm{\lambda}_T$.  We employ a \texttt{TelFit} model, though any precomputed synthetic telluric model will work.  The \texttt{TelFit} model does not contain any continuum sources of opacity, so we can skip the flattening procedure described in Equation \ref{eqnFlattening}.  We orchestrate the same initialization and line finding as in the stellar models and obtain a coarse clone.

The number of pixels in the telluric model can be chosen at the time of running a \texttt{TelFit} model.  Here we choose a spectral resolution $R\sim10^6$, adequate for resolving narrow telluric lines, and yielding about 2 million pixels across the entire HPF bandwidth.  This pixel sampling is about $6\times$ finer than the native PHOENIX pixel sampling.  The number of telluric lines depends on the atmospheric properties, in particular, the local surface temperature $T_\oplus$ and relative humidity $RH$.  For a surface temperature of $T=290 \mathrm{K}\; (62^\circ~\mathrm{F})$, relative humidity of 40\%, and typical conditions for McDonald Observatory, we anticipate 3615 telluric lines across the entire HPF bandwidth, distributed as shown in the bottom panel of Figure \ref{fig_Nlines_vs_teff}.

One guiding principle departs from the stellar case: telluric lines do not require future-proofing for large radial velocity shifts, hypothetically allowing us to reduce the number of pixels needed for a wingcut.  Small radial velocity shifts are possible due to bulk motions in the Earth's atmosphere, but those bulk motions should be much smaller than the speeds of stars towards and away from Earth.  So we can hypothetically tolerate a much smaller wing cut for telluric lines.  In practice, telluric lines can be saturated, and accurately cloning the resulting broad telluric wings still benefits from $N_{\rm cut}\sim6000$ pixels, comparable to the stellar scenario.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.48\textwidth]{example-image-a}
    \caption{Pre-computed TelFit model cloned with \texttt{blase}.}
    \label{fig_telluric_clone}
\end{figure}

\authorcomment1{Figure \ref{fig_telluric_clone} shows the telluric clone for the same region as Figure \ref{fig_cloned_spectrum_demo}.}

We optimize the sparse telluric clone, achieving comparable computational speed as the PHOENIX cloning task.  We are left with $\mathsf{T}_{\rm clone}(\bm{\lambda}_T)$, the tunable telluric clone model evaluated at its original native coordinates.


\section{Cloning Performance}

We compute the residual $\mathsf{R}(\bm{\lambda}_S)$ of native-minus-cloned PHOENIX model, illustrated in the bottom panel of Figure \ref{fig_cloned_spectrum_demo}. We see an RMS residual of 1.2\%/pixel at native resolution.  The telluric clone shows a comparable level of performance.  These residuals tend to pile up in local symmetrically balanced clusters that get cancelled out once convolved with coarser instrumental line profiles, so their overall effect at instrumental resolution is typically negligible.  We identify three main categories of cloning flaws that may not be negligible depending on the science application.

The first---and expected---source of large residuals is simply missing line opacity due to our finite prominence threshold. Lines with prominence less than $P_{\rm rom}$ yield residual notches with strengths comparable to $P_{\rm rom}$. Including smaller prominence lines by lowering $P_{\rm rom}$ produces smaller residuals, at the tradeoff of computing more lines and yielding higher computational cost.  But at some point, turning down $P_{\rm rom}$ yields diminishing returns, as other imperfections provide a noise floor.  We have experimentally determined this noise floor to occur near $P_{\rm rom}=0.01$.

Second, another anticipated flaw occurs in the line cores of relatively narrow lines, where the pseudo-Voigt profile becomes a poor approximation of the exact Voigt profile.  The cloned model tends to overestimate the flux at the core and underestimate the flux along the slopes of the lines.

Finally, and most perniciously, a large category of residuals appear near the wings of the deepest and broadest lines---such as Hydrogen and neutral alkali metal lines.  The true lines exhibit advanced lineshapes, such as non-Lorentzian line wings that are not captured with the overly simplistic Voigt line profile.  Figure \ref{fig_zoom_cloning_performance} highlights super-Lorentzian line wings around a line at 8691~\AA.  Narrow lines devolve into missing line wing opacity, the favored tradeoff when the continuum estimate's poor performance outweighs the pain of a narrow-but-tolerably-small spike. This flaw can be seen where a line initialized at 8692.5~\AA~ and another pair of lines at 8690.0 all melt into line wings.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/blase_clone_demo_zoom.png}
    \caption{Zoom in of the region between the vertical gray bars in Figure \ref{fig_cloned_spectrum_demo}. The cloned model has 7 spectral lines describing the 400 pixels in this 4 \AA~ chunk.  The native PHOENIX pixel sampling can be see as the boxy steps in both the native and cloned model.  }
    \label{fig_zoom_cloning_performance}
\end{figure}

Cloning telluric lines suffers from one additional problem.  Telluric lines can be extremely deep, exhibiting almost vanishing transmission with saturated line cores common in-between the atmospheric windows that define the $I$, $J$, $H$, and $K$ bands.  The \texttt{blas\'e} method can cope with these saturated lines, but often it treats nearby-and-blended saturated lines as one single line.  This glomming together of lines has little practical effect since few astronomical practitioners can make use of such profoundly saturated data.


\section{Semi-empirical models with transfer learning techniques}\label{transferLearn}


The cloned models already represents a useful intermediate product: the distillation of $N_s=335,849\times2$ pixel flux values and their wavelength coordinates into a more compact quartet of properties for a list of $<10,000$ spectral lines, a dimensionality reduction of $18\times$ for the cost of 1.2\%/pixel in accuracy.  So the process so far can be myopically viewed as a physics-informed compression algorithm.  But the cloned model serves as a mere stepping stone in our principal quest: the comparison of models to real data.

\subsection{Augmenting the stellar clone with radial velocity and rotational broadening }
Real stars possess two key extrinsic properties.  Rotational broadening $v\sin{i}$ and radial velocity $RV$ depend on the observer's viewing location. We follow \citet{czekala15} by emphasizing the qualifier \emph{extrinsic}, to distiguish between stellar \emph{intrinsic} properties, such as $T_{\mathrm{eff}}, \log{g},\mathrm{and\,} [\mathrm{Fe}/\mathrm{H}]$.  Intrinsic properties appear the same from any viewing location---at least for stars with isotropic surfaces---while extrinsic properties do not.  The distinction is important because the extrinsic terms act as simple convolutions and translations to the cloned spectrum, and can be treated after the cloning procedure.  We therefore define an augmented model, which we designate the ``extrinsic'' model, $\mathsf{S}_{\rm ext}$:

\begin{eqnarray}
    \mathsf{S}_{\rm ext}(\bm{\lambda}_Z) = \mathsf{S}_{\rm clone}(\bm{\lambda}_Z - \frac{RV}{c}\lambda_\mathrm{c}) * \zeta \left(\frac{\bm{v}}{v\sin{i}}\right) \label{eqn_convolution}
\end{eqnarray}

where $\zeta$ is the convolution kernel for rigid body rotation \citep[\emph{e.g.}][]{2022ApJS..258...31K}, $\bm{v}$ is the spectral axis represented as relative velocity coordinates, and $*$ denotes the convolution operator.  

Most autodiff-aware \texttt{convolve} operators act in pixel-space, approximating kernels as numerically sampled functions.  There exists a special exponential spectral sample spacing that allows the convolution operators to work out of the box for rotational broadening.  The design of \texttt{blas\'e} permits the stellar and telluric models to be re\"evaluated at any wavelength coordinate vector, provided that it adequately samples the underlying lines.  We, therefore, change the sampling from the native stellar and telluric wavelength coordinate grids, $\bm{\lambda}_S$ and $\bm{\lambda}_T$, to this special exponentially sampled wavelength grid, denoted with the subscript $Z$:

\begin{eqnarray}
    \bm{\lambda}_Z = \lambda_0  \exp{\frac{\bm{v}-v_0}{c}}
\end{eqnarray}

where $\bm{v}-v_0$ is the velocity vector going from zero to the velocity associated with the largest wavelength, with \emph{linear} spaced velocity samples.  We choose a sampling in velocity space of 0.5 km/s, which corresponds to about $10\times$ finer than the instrumental resolving power of HPF, and delivers a minimum and maximum wavelength spacing of 0.013 and 0.024 \AA$/\text{pixel}$ respectively for the HPF bandwidth.

Operationally, the radial velocity shift $RV$ gets applied to the line center positions rather than scaling the entire wavelength grid point coordinates, $\bm{\lambda}_Z$. This choice yields a convenience: it cleanly makes the $RV$ autodiff-aware, meaning that an infinitesimal change to the RV value can be sensed through backpropagation by affecting only the line center positions.

The $\mathsf{S}_{\rm ext}$ spectrum is shown in Step 2 of Figure \ref{blase_flowchart}.  There is currently no equivalent post-processing of the telluric spectrum $\mathsf{T}$.  As mentioned previously, we assume motions of the Earth's atmosphere are much less than the desired stellar radial velocity precisions.  However, demanding Extreme Precision Radial Velocity (EPRV) exoplanet searches may need to consider minuscule systematic $RV$ shifts and broadening of the telluric templates, arising from the turbulent and bulk motions of the Earth's atmosphere.  A $\mathsf{T}_{\rm ext}$ could hypothetically be implemented in \emph{blas\'e} to achieve these strenuous precision demands.


\subsection{Joint Stellar and Telluric Model}
Figure \ref{blase_flowchart} shows a visual guide to all the steps in \texttt{blase}.  We have arrived at what may be the most intriguing-and-yet-simple of these steps: we simply multiply the rotationally-broadened-and-$RV$-shifted stellar model by the telluric transmission:

\begin{eqnarray}
    \mathsf{M}_{\rm joint} = \mathsf{S}_{\rm ext}(\bm{\lambda}_Z) \odot \mathsf{T}_{\rm clone}(\bm{\lambda}_Z)
\end{eqnarray}

It is only at this stage that we may apply the instrumental broadening kernel.  The instrumental resolving power, $R$, acts as a convolution with a Gaussian line profile of width $\sigma=\frac{c}{2.355 R}$.  Real astronomical instruments usually have wavelength-dependent resolving power, which complicates the implementation for high-grasp spectra.  The extent to which this effect matters will depend on the science application.  For now, \texttt{blas\'e} simply assumes a fixed resolving power.

Notice that the order of Steps 3 and 4 in Figure \ref{blase_flowchart} cannot be swapped.  Mathematically speaking, multiplication and convolution do not commute.  While the distinction may seem negligible, it matters at the level of precisions sought in EPRV applications (Suvrath Mahadevan, \emph{priv. comm.}).  Water vapor lines in our own atmosphere can ``beat'' with water vapor in the spectrum of, say, an M-dwarf atmosphere.  The systematic telluric mis-cancelation would imbue a Moir\'e pattern of residuals that is most acute for sources with sharp lines, namely low projected rotational broadening ($v\sin{i}\sim\frac{c}{2R}$).  The approach in \emph{blas\'e} may therefore unlock a level of telluric calibration that has evaded previous efforts.  

As noted, typical data-pixel sampling $\bm{\lambda}_D$ is much coarser than the model pixel sampling $\bm{\lambda}_Z$.  We therefore resample the model to the data spectrum in the following way.  We evaluate the joint model $\mathsf{M}_{\rm joint}$ at all of the super-resolution wavelength coordinates, and then compute the mean value of those pixels within the bounds of each coarse data pixel.  The resampling procedure is autodiff-aware: the same clusters of high resolution coordinates map to the same data pixel coordinates, no matter what the $RV$ is.  The $RV$ only dictates what flux values are realized within those pixel bounds.

The final forward model for \emph{blas\'e} is designated simply as $\mathsf{M}$ without subscripts to emphasize that we have achieved the desired goal of a plausible end-to-end physics-informed yet highly flexible forward model for each datum in the 1D observed spectrum:

\begin{eqnarray}
    \mathsf{M}(\bm{\lambda}_D) = \resample{\Big[\mathsf{M}_{\rm joint}(\bm{\lambda}_Z) * g(R) \Big]} \label{eqn_final_model}
\end{eqnarray}

where $g$ is the Gaussian instrumental convolution kernel and the \texttt{resample[]} operation indicates the average of model pixels that fall within each data pixel's red and blue boundaries.

\subsection{Regularization}

Equation \ref{eqn_final_model} has $\sim21,000$ tunable parameters from the star, $\sim9,000$ tunable parameters from the Earth's atmosphere, plus $v\sin{i}$ and $RV$.  That adds up to about $30,002$ model parameters.  The resolving power may also be treated as tunable if it is not known or varies slightly with \emph{e.g.} seeing, slit-or-fiber illumination, or instrumental configuration: $30,003$.

It may appear desirable to simply optimize all of these parameters in a \emph{laissez-faire} manner, allowing them all to take on whatever value the data dictates.  Such a stratagem would overfit the data, resulting in unphysically perverse lineshapes that do not reflect the air of reality we aspire to impose on our synthetic spectral models.  Lines would haphazardly fit noise-spikes, and conspire together to warp spectral shapes in unexpected ways.  This overfit model may suit some rare purposes.  But most of the time, we prefer to strike a better balance in the bias-variance tradeoff.

We apply some amount of \emph{regularization}, a restriction on the allowed values the model parameters can take on.  Fortunately, we have a firm theoretical basis to justify this regularization.  We believe our precomputed synthetic spectral models are \emph{quasi-statically correct}: the predicted spectra resemble the unobserved ``True spectrum'' with lines in the correct place, but just with the wrong area under the curve.  This statement may stem from the fact that it is easier to predict the mere existence of some energy transition of atoms and molecules than it is to predict their transition rates, abundances, temperature and pressure effects, and all the other line strength effects that flow down to how much light a line ultimately absorbs in a stellar atmosphere.

The degree of regularization will control the extent of overfitting or underfitting. The most extreme regularization---the antithesis of the \emph{laissez-faire} scenario---would yield a model too rigid to respond to the data at all, yielding a model entirely unchanged from the cloned PHOENIX and \texttt{TelFit} models, the extreme end of underfitting.  So regularization constitutes the only hyperparameters worthy of tuning in \texttt{blas\'e}.  The choice of how to set the regularization is problem-specific.  We default to the following choice. We fix all line parameters except for amplitude, which receives an L1 loss-- namely we penalize the absolute value of departures from an amplitude's starting place:

\begin{eqnarray}
    \mathcal{L}_{reg} &\equiv& \sum_{j=1}^{N_\mathrm{lines}} \Big|\frac{\ln{a_j} - \ln{\hat{a}_j}}{\Lambda}\Big|
\end{eqnarray}

where the hat notation demarcates the amplitudes obtained from cloning, \emph{i.e.} the ``initial, theory-inspired amplitude''.  We assign $\Lambda\sim5$.  The total, overall loss then becomes:

\begin{eqnarray}
    \mathcal{L}_{tot} = \mathcal{L}_{MSE} + \mathcal{L}_{reg}
\end{eqnarray}

This weak degree of regularization has the effect of permitting refinement of only the most conspicuous data-model mismatches; the weakest lines do not bother to move from their initial state because doing so would penalize the regularization without enough reduction of the overall loss.  The extrinsic $v\sin{i}$ and $RV$ have no regularization, but in practice, they need to be initialized close to their plausible values.

\subsection{Comparison to HPF data of WASP 69}

Figure \ref{fig_WASP69_demo} shows an example of comparing HPF data of WASP 69 to the closest PHOENIX template, post-processed to the HPF resolution and sampling.  Figure \ref{fig_WASP69_transferred} shows the same data spectrum compared to a pixel-level \emph{blas\'e} model regularized to the clone in Figure \ref{fig_cloned_spectrum_demo}.  The latent high-resolution model for WASP 69 is therefore the transfer-learned model, unadjusted for the extrinsic and instrumental properties.  This semi-empirical model is shown in Figure \ref{fig_WASP69_regularized}.


\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/blase_WASP69_demo.png}
    \caption{WASP 69 observed with HPF, compared to a $T_{\mathrm{eff}}=4700\;$K,  $\log{g}=4.5$ solar metallicity PHOENIX model warped to $v\sin{i}=2.2$~km/s, $RV=-9.6$~km/s, and HPF resolving power.}
    \label{fig_WASP69_demo}
\end{figure*}

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/blase_WASP69_regularized.png}
    \caption{Semi-empirical model of WASP 69 transfer-learned with \texttt{blas\'e}, employing a regularization prior on the learned amplitudes.}
    \label{fig_WASP69_transferred}
\end{figure*}

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/blase_super_resolution_template.png}
    \caption{The native-resolution semi-empirical model transfer-learned from WASP 69 HPF data. The revised model can be viewed as a super-resolution deconvolution of the HPF spectrum.}
    \label{fig_WASP69_regularized}
\end{figure*}


\subsection{Comparison to IGRINS spectra of a T6 Ultracool Dwarf}

Here we demo on IGRINS data from \citet{2022MNRAS.tmp.1421T}.

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/IGRINS_T6_sonora_demo.png}
    \caption{Before, middle, and after the blase transfer learning.  }
    \label{fig_IGRINS_sonora_demo}
\end{figure*}


\subsection{End-to-end validation with injection/recovery tests}\label{secInjRec}
\lipsum[2-3]

\section{Discussion}\label{secDiscuss}

\subsection{Comparison to existing frameworks}

Several astronomical spectral frameworks share similar aims as \texttt{blas\'e}.  These existing frameworks will have enduring value for the wide range of problems in the field of stellar spectroscopy.  Here we scrutinize the differences among some of these approaches to clarify how this work fits in.

The \texttt{specmatch} synthetic template matching tool produces noise-free nearest neighbor templates given an input spectrum \citep{2015PhDT........82P}.  Several practical barriers limit the accuracy of using precomputed synthetic spectral models alone. First and foremost, real stars are usually more complicated than our simplified models of them. Real spectra often vary over more dimensions that our models do.  Conspicuous examples of these hidden variables can be found in protostars: starspots, accretion veiling, dust extinction, and magnetic Zeeman splitting. Jointly modeling all of these phenomena alongside the intrinsic stellar photosphere is challenging.


The empirical version, \texttt{specmatch-emp} \citep{2017ApJ...836...77Y} matched spectra better than the synthetic templates, but is still too rigid for some applications and requires the assembly of hundreds of standardized high signal-to-noise-ratio templates, ideally with low intrinsic rotational broadening.  Such a large number of high-quality templates with high resolving power and low $v\sin{i}$ has not yet been established in the near-infrared.

The \texttt{wobble} framework \citep{2019AJ....158..164B} modernized the construction of high-SNR templates to account for temporally variable telluric lines. The tool requires dozens of high-SNR spectra acquired at a range of Barycentric Earth Radial Velocities (BERVs).  The final telluric-free combined spectrum would still have to be compared to models for absolute calibration or can be used out-of-the-box for precision relative RVs.  The \texttt{wobble} framework also pioneered the off-label application of automatic differentiation frameworks---in this case \texttt{TensorFlow}---towards their physically-motivated use in stellar spectra.  \texttt{blas\'e} can be viewed as an evaluable and interpretable super-resolution version of \texttt{wobble}, that accepts more bias in the bias-variance tradeoff.

The \texttt{starfish} framework \citep{czekala15} provides a robust likelihood function for data-model comparisons and retires many of the problems in this domain.  \texttt{starfish} pioneered the use of whole-spectrum fitting with resilience to model imperfections by addressing the problem of what to do when the underlying atomic and molecular data was wrong or approximate or missing.  It has been extended to inferring starspot physical properties \citep{2017ApJ...836..200G}, measuring veiling in Class 0 protostars \citep{2018ApJ...862...85G}, and quantifying imperfections in brown dwarf models \citep{2021ApJ...921...95Z}.  The Spectral Inference Crank \citep[\texttt{sick},][]{2016ApJS..223....8C} shares similar aims as \texttt{starfish}, and provides additional useful grid search capabilities.

For very large bandwidths and very many spectral lines, the problem of identifying and cataloging line imperfections essentially becomes a book-keeping and continuum assignment problem.  \texttt{blas\'e} and \texttt{starfish} provide different strategies for orchestrating the line-mismatch identification procedure, with each route having tradeoffs depending on the application.


\section{Conceivable Improvements, Extensions, and Limitations}\label{secFutureWork}

The scaffolding of \texttt{blase} is designed in a way to promote extensibility, so the future of the framework is bright.  Here we enumerate some planned or conceivable extensions to the framework that would unlock new science arenas.  We order the presentation of these extensions from our perceived ease of implementation, from easiest to hardest.

\subsection{Exact instead of pseudo Voigt Profile}
We currently employ the pseudo-Voigt profile for its low computational cost.  We have a prototype exact-Voigt-Hjerting implementation following \citet{2022ApJS..258...31K}.  We coarsely estimate that moving to this exact-Voigt implementation could decrease some residual regions by $\sim 30\%$, while increasing the computational cost by more than $10\times$ over the existing pseudo-Voigt approximation.  The exact-Voigt-Hjerting implementation still outbids the higher cost of a direct numerical convolution of a Gaussian and Lorentzian profile.



\subsection{Addressing the pseudo-continuum with Gaussian Process regression}

We currently assume the input spectra are adequately normalized to the continuum.  We have a few options to relax this assumption.  We could  simply tune the $\mathsf{P}$ term that represents the wavelength-dependent pre-factor to Equation \ref{equation1}.  Tuning $\mathsf{P}$ would correct for large-scale imperfections in the otherwise-fixed continuum flattening procedure.  This change would be easy and effective, but has some challenge with model selection and flexibility: how to set the polynomial order to avoid over- and under-fitting.  Gaussian Processes (GPs) offer many advantages for continuum fitting \citet{czekala15}.  In short, a GP-likelihood relaxes the assumption that the continuum has been perfectly normalized, in favor of the more realistic statement ``the continuum has been coarsely normalized, with some characteristic-but-as-yet-unknown correlation and scale length and amplitude of the imperfections''.  That statement translates to the following modification to Equation \ref{simpleLikelihood}:

\begin{eqnarray}
    \mathcal{L} =  \frac{1}{2}\mathsf{R^\intercal} \mathsf{C}^{-1} \mathsf{R} +\frac{1}{2}\ln{\det{\mathsf{C}}} \label{GPLikelihood}
\end{eqnarray}

where we introduce the covariance matrix $\mathsf{C}$, with its associated kernel and collection of typically 2-3 parameters.  We anticipate that this GP likelihood would have the greatest impact on stellar stars with significant band-heads and line-blanketing: spectra with a so-called ``pseudo-continuum''.  M-dwarfs and brown dwarfs fall into this challenging category.

The main demerit of moving to a GP-likelihood is computational cost.  Fortunately, a few efficient autodiff-aware implementations of GPs exist. The \texttt{celerite} algorithm \emph{celerit\'e} \citep{2017AJ....154..220F} has an exact backpropation implementation \citep{2018RNAAS...2...31F} that scales linearly with the number of data points.  The \emph{celerit\'e} algorithm does not currently have a PyTorch implementation.  The \texttt{GPyTorch} framework \citep{2018arXiv180911165G} has a large category of approximate and exact GPs that could be straight-forwardly dropped into \texttt{blas\'e}.  Even still these GPs could increase the computation cost by of order $10\times$.


\subsection{Minibatches and Stochastic Gradient Descent}

Currently, each training epoch sees the entire dataset, a setup dubbed \emph{full-batch} gradient descent.
An alternative scheme allows training with only a portion of the entire dataset at a time in \emph{minibatches}.  The massive data volumes in modern Neural Network applications cannot fit into the GPU memory, so minibatches are a necessity.  Our meager 1 MB dataset can easily fit into the GPU memory, but our model can be large if we have a large number of pixels or lines or both.
So while minibatches may not be required due to data size limitations they may be useful for particularly large models.
Minibatches also act as a form of regularization, the principal source of stochasticity in the Stochastic Gradient Descent algorithm, which tends to have better convergence than full-batch Gradient Descent \citep{2016arXiv160904747R}.

We experimented with minibatches by assembling and evaluating only a portion of the dense $\bar{\bm{F}}$ matrix at a time, in minibatches. The choice to evaluate only a portion of lines at a time would mean the model is inaccurately evaluated at all wavelength pixels. Instead, we choose to evaluate all lines, but only on a random subset $N_{\mathrm{batch}}$ of the total pixels $N_s$, so that the model can eventually converge to exact at those points. All lines are allowed to update at each glimpse of a minibatch, but many lines with cores far from minibatch pixels will provide only weak information about how the loss scalar changes for their parameters.

Overall minibatches as implemented above performed worse than the sparse implementation, with both lower accuracy and slower computation time.

\subsection{Broad lines and advanced lineshapes}

Some lines---such as those arising from hydrogen, sodium, potassium, and others---have extremely broad line wings, approaching larger than the $\sim6000$ pixels we allocate for the sparse implementation. These special lines should be handled separately from the weak lines, both from a computational performance perspective and an accuracy perspective.

Extremely broad lines will exhibit truncation effects if the sparse window is small compared to the line wing size. The truncation effects will look like tophat functions severing the asymptotic wings, imbuing artificial step function kinks in the emulated spectrum. We can afford to increase the sparse window on a few, say $N_{broad}\sim20$ of the broadest spectral lines. We then construct and evaluate the entire dense matrix for those lines: $\sim 330\;000 \times 20$. The number of FLOPS in each category scales as about 6 Million for the 20 broad and dense lines versus about 36 Million for the sea of about 7000 narrow and sparse lines, depending on the exact choices for wing cuts and the number of lines.

One could introduce advanced lineshapes for these $\sim20$ broad lines, perturbing the Voigt line-wings with a smooth wavelength-dependent correction term $\mathsf{G}$:

\begin{eqnarray}
    \mathsf{\tilde{V}(\bm{\lambda})} &=& \mathsf{V} \odot \mathsf{G}\\
    \mathsf{G} &=& 1 + (e^{\alpha_j} - 1) \cdot \mathcal{S}\left(\frac{|\bm{\lambda}-\lambda_{c,j}| - \lambda_{t, j}}{b_j}\right)
\end{eqnarray}

where $\odot$ is again the element-wise product (\emph{a.k.a} Hadamard product), $\mathcal{S}$ is the sigmoid function, and we have introduced three new tunable parameters for each of the $j$ broad lines; $\lambda_t$ is the truncation wavelength, $b$ is a scale parameter for how slowly or how rapidly in wavelength-space the transition from non-Lorentzian proceeds, and $\alpha$ is a possibly negative stretch parameter that controls whether the line wing is sub- or super- Lorentzian.

This functional form has a few advantages. It is smooth. The smoothness of the transition is controlled by a tunable parameter, $b$. It can handle either sub- or super-Lorentzian shapes. It is roughly based on the $\chi$-factor\authorcomment1{citation to Hartmann et al. 1989, etc.}. In the limit $\lim_{a\to0} \mathsf{G}$, the lineshape becomes exactly Lorentzian. The sigmoid is efficiently implemented in PyTorch.
Finally, it enforces that the perturbation only produces absorption and not emission profiles.

\subsection{Wavelength dependent limb darkening}
Currently the extrinsic model step possesses up to four parameters: the $v\sin{i}$ and $RV$, and 2 optional parameters for limb-darkening.  These four parameters may adequately parameterize a star with a uniform stellar disk.  Extremely high signal-to-noise-ratio spectra of rapidly rotating stars may require additional flexibility.  The limb darkening is generally wavelength-dependent, and so a pan-chromatic spectrum may require a different limb darkening from the blue end to the red end.  The limb darkening may instead depend on physical properties of the spectral line formation, such as physical depth of formation, and so the extent of limb-darkening may jump haphazardly from line-to-line-to-line, rather than as a predictably smooth function across wavelength.  \texttt{blas\'e} could be built to handle such a seemingly pathological scenario by adding a vector of limb darkening parameters, one for each line.  One would have to regularize the fits with some typical limb darkening and a heuristic penalty for departures from this mean.

\subsection{Doppler Imaging}
The fixed $v\sin{i}$ approximation breaks down for stars with large-scale surface features.  Doppler imaging attempts to reverse engineer the surface map from the extent to which observed line profiles depart from a pristine rotational broadening kernel.  This reverse engineering step suffers from a vast number of geometrical degeneracies, but still provides useful constraints on stellar surfaces.  We emphasize a distinction between A) longitudinally symmetric surface features, and B) longitudinally asymmetric surface features.  Most radial velocity practitioners think about the latter, since longitudinally asymmetric surface features imbue changing-in-time skewness to the line profiles, causing radial velocity perturbations easily detectable in radial velocity time series.  These confound exoplanet searches.

Longitudinally symmetric surface features do not change as the star rotates on its axis.  The existence of these features manifest as static-in-time \emph{kurtosis} of the spectral line.  For example, a hypothetical non-emitting (black) polar starspot exhibits a deficit in flux at the line core, resulting in less zero-velocity flux than its homogeneous counterpart.  A dark zonal band results in equal-sized bites out of the red- and blue- sides of the line.

It is easier to reverse-engineer longitudinally asymmetric features than longitudinally symmetric ones, since we assume that we occasionally catch a glimpse of the spot-free limbs and their pristine line profiles.
The latter requires exact knowledge of the underlying spectral template.  Isolated, deep, well-calibrated spectral lines constitute the only practical scenario where exact knowledge can plausibly be claimed.  Isolated spectral lines may be scarce or absent for M-dwarfs and brown dwarfs where lines blend ostensibly in an inseparable way, counfounding Doppler imaging.

\texttt{blas\'e} offer a new approach to Doppler imaging that may overcome these historical limitations by simultaneously fitting both the imperfections in the underlying spectrum and its line profile perturbations.  This approach is analogous to the linearized model in \citet{2021arXiv211006271L}, but with the benefit of also handling non-linear properties of the spectrum such as line widths, shapes, and locations, while also handling telluric contamination.  Hypothetically the \citet{2021arXiv211006271L} approach could be partially absorbed into \texttt{blase}, or vice versa, though such a merger may be complicated to implement.


\subsection{RV jitter and microturbulence}

Already \texttt{blas\'e} is equipped to fit every single line with its own systemic $RV$, by tuning the line center position $\lambda_\mathrm{c}$ at each training epoch.  There exists both empirical evidence and some theoretical motivation that $RV$ jitter varies from line-to-line \citep{2018A&A...620A..47D}.  Importantly, the extent of this line-by-line RV jitter could be predicted in part by the depth of line formation.  A future extreme-precision-RV (EPRV) version of \texttt{blase} could leverage this information.  As an example, the depth-of-formation for all (or a subset) of lines could be obtained and associated with each spectral line.  A regularization scaling term could be introduced to allow the line positions to vary, but only in proportion to their depth of formation.


\subsection{Using native line lists rather than clones}
For many practitioners, the choice to clone precomputed synthetic models in the first place may seem roundabout: ``Why not just use the line lists?''.  Adopting the line lists would have many advantages: it would provide chemical and molecular provenance tags.  Metadata associated with the quality of the atomic and molecular data could be used to assign physics-informed regularization.  Many other benefits would effortlessly accrue from adopting the native line lists.  The \texttt{FAL} project (Cargile et al. \emph{in prep}) follows such a principled prescription.

There exists at least a few demerits of adopting the line lists, and therefore supporting the \emph{blase} strategy.  First, these line lists need to undergo expensive multi-level radiative transfer calculations in order to obtain their amplitudes, so adopting the line lists would mean a laborious and computationally expensive pursuit simply to get close to what has already been computed.  Second, as the effective temperature scales to ultracool dwarfs (Figure \ref{fig_Nlines_vs_teff}) the number of lines sky-rockets, tending towards the billions for T-dwarfs.  The methane line list alone \censorbox{Hargraves et al 2022} represents a prohibitive data volume.  The ExoJAX and Radis \censorbox{Radis paper} libraries offer a breakthrough solution to the voluminous line list problem.  Even still, \emph{blase} deals with the less pure but more practical ``pseudo line'' that gets closer to the astronomical observables anyways, and offers a middle ground between the extremes of interpretability and performance.


\section{Conclusions}
\lipsum[5-6]



\begin{acknowledgments}

    These results are based on observations obtained with the Habitable-zone Planet Finder Spectrograph on the HET. The HPF team was supported by NSF grants AST-1006676, AST-1126413, AST-1310885, AST-1517592, AST-1310875, AST-1910954, AST-1907622, AST-1909506, ATI 2009889, ATI-2009982, and the NASA Astrobiology Institute (NNA09DA76A) in the pursuit of precision radial velocities in the NIR. The HPF team was also supported by the Heising-Simons Foundation via grant 2017-0494.

    The Hobby-Eberly Telescope (HET) is a joint project of the University of Texas at Austin, the Pennsylvania State University, Ludwig-Maximilians-Universit\"at M\"unchen, and Georg-August-Universit\"at G\"ottingen. The HET is named in honor of its principal benefactors, William P. Hobby and Robert E. Eberly.

    This material is based upon work supported by the National Aeronautics and Space Administration under Grant Numbers 80NSSC21K0650 for the NNH20ZDA001N-ADAP:D.2 program,
    and 80NSSC20K0257 for the XRP program issued through the Science Mission Directorate.

    We acknowledge the National Science Foundation, which supported the work presented here under Grant No. 1910969.

    This research has made use of NASA's Astrophysics Data System Bibliographic Services.

\end{acknowledgments}


\facilities{HET (HPF)}

\software{ pandas \citep{mckinney10},
    matplotlib \citep{hunter07},
    astropy \citep{exoplanet:astropy13,exoplanet:astropy18},
    exoplanet \citep{exoplanet:joss}, %celerite?
    numpy \citep{harris2020array},
    scipy \citep{2020SciPy-NMeth},
    ipython \citep{perez07},
    starfish \citep{czekala15},
    seaborn \citep{Waskom2021},
    pytorch \citep{2019arXiv191201703P}}


\bibliography{ms}


\clearpage

\appendix
\restartappendixnumbering

\section{Log flux scaling mode} \label{appendixLogScale}

Here we illustrate how \texttt{blas\'e} gets altered when applying the logarithmic flux pre-processing step.  First, we compute the natural log of the flux directly on the precomputed synthetic spectrum in its absolute flux scaling and native pixel sampling:

\begin{eqnarray}
    \ln{\mathsf{S}} = \ln{\mathsf{S}_{\rm abs}} - \ln{\mathsf{B}} - \mathsf{P}
    \label{eqnlogFlat}
\end{eqnarray}

We simply ``rebrand'' $\mathsf{P}$ as residing in logarithmic flux units, and disregard it since it is largely a nuisance parameter anyways.  We then treat the \texttt{blas\'e} clone model as a sum of opacities, retaining the Voigt profile:

\begin{eqnarray}
    \ln{\mathsf{S}_{\rm clone}} = -\sum_{j=1}^{N_{\mathrm{lines}}} a_j \mathsf{V}_j \label{equationOpacitySum}
\end{eqnarray}

Here, the $a_j$'s have also been slightly rebranded from their meaning in Equation \ref{equation1}.  We still want to enforce only absorption lines---and not spurious emission lines---so we use the sample trick of sampling the $a_j$'s in log and then exponentiating them to get guaranteed positive values.  Note that Equations \ref{equation1} and \ref{equationOpacitySum} carry modified meanings for the Voigt profile.  Specifically, Equation \ref{equation1} can be viewed as the Taylor Series expansion for \ref{equationOpacitySum} in the limit of small opacities:

\begin{eqnarray}
    e^{-a_j \mathsf{V}_j} \approx (1-a_j\mathsf{V}_j) \label{eqnTaylor}
\end{eqnarray}

Both equations are approximate. A real stellar atmosphere's lineshape arises from a sum of disparate Voigt profiles weighted along a nonuniform column of gas, whereas here we have assumed the column of gas is approximated as a single uniform isothermal backlit layer.  A sum of unlike-Voigt profiles is not exactly equal to any single Voigt profile.  Theoreticians may resonate with this more ``first principles'' representation, while data practitioners may find Equation \ref{equation1} more natural, so to some extent the choice is a matter of taste.

The sparse matrix gets rebranded as filled with opacity values, instead of log-fluxes, but operationally remains the same. All subsequent steps operate on the summed-and-exponentiated opacities, behaving identically to their linear counterparts.  For example, we exponentiate before computing the residuals and data-model comparison, $\mathsf{R} = e^{\ln{\mathsf{S}}} - e^{\ln{\mathsf{S}_{\rm clone}}}$.



% Table
\begin{deluxetable}{cp{10cm}}
    \tabletypesize{\scriptsize}
    \tablecaption{Notation used in this paper\label{table2}}
    \tablehead{
        \colhead{Symbol} & \colhead{Meaning}
    }
    \startdata
    \hline
    \multicolumn{2}{c}{Spectra}\\
    \hline
    $\bm{\lambda}_S$ & Native wavelength coordinates of the precomputed stellar spectrum\\
    $\bm{\lambda}_T$ & Native wavelength coordinates of the telluric spectrum\\
    $\bm{\lambda}_D$ & Native wavelength coordinates of the data spectrum\\
    $\mathsf{S}_{\rm abs}$ & Flux values of the precomputed synthetic stellar spectral model $\bm{\lambda}_S$\\
    $\mathsf{B}$ & Blackbody of temperature $T_{\mathrm{eff}}$ to coarsely normalize $\mathsf{S}_{\rm native}$\\
    $\mathsf{P}$ & Smooth polynomial to refine continuum-normalization\\
    $\mathsf{S}$ & Continuum normalized augmentation of $\mathsf{S}_{\rm abs}$\\
    $\mathsf{T}$ & Transmission values of the precomputed synthetic telluric model \\
    $\mathsf{D}$ & The observed data spectrum flux values\\
    $\bm{\epsilon}$ & The estimated uncertainties in the data spectrum\\
    $\mathsf{S}_{\rm clone}$ & Evaluable and tunable cloned flux model of $\mathsf{S}$\\
    $\mathsf{T}_{\rm clone}$ & Evaluable and tunable cloned transmission model of $\mathsf{T}$\\
    $\mathsf{S}_{\rm ext}$ & An augmentation of $\mathsf{S}_{\rm clone}$ with $v\sin{i}$ convolution and $RV$ translation\\
    $\mathsf{M}_{\rm joint}$ & The joint stellar and telluric model: $\mathsf{S}_{\rm ext} \odot \mathsf{T}_{\rm clone}(\bm{\lambda}_S)$  \\
    $\mathsf{M}$ & Joint model convolved
    with instrumental kernel and resampled to $\bm{\lambda}_D$\\
    $\mathsf{R}$ & The residual spectrum between a pair of inputs, \emph{e.g.} $\mathsf{D} - \mathsf{M}$\\
    $\bm{v}$ & The spectral coordinate axis $\bm{\lambda}$ expressed as a velocity difference\\
    \hline
    \multicolumn{2}{c}{Line properties}\\
    \hline
    $\lambda_{\mathrm{c},j}$ & Line center position of the $j^{th}$ spectral line\\
    $a_j$ & Gaussian line profile amplitude of the $j^{th}$ spectral line \\
    $\sigma_j$ & Gaussian line profile scale of the $j^{th}$ spectral line\\
    $\gamma_j$ & Lorentzian line profile half width of the $j^{th}$ spectral line\\
    $\mathsf{V}_j$ & The Voigt profile of the $j^{th}$ spectral line \\
    $\bar{\bm{F}}$ & The dense $(N_{\rm lines} \times N_{x})$ matrix of all line fluxes stacked vertically \\
    $\hat{\bm{F}}$ & The sparse $(N_{\rm lines} \times N_{\rm sparse})$ matrix of all line fluxes stacked vertically \\
    $\zeta$ & The rotational broadening convolution kernel\\
    $g$ & The instrumental broadening convolution kernel, typically a Gaussian\\
    \hline
    \multicolumn{2}{c}{Scalars}\\
    \hline
    $N_{\rm lines}$ & Number of spectral lines \\
    $N_{x}$ & Number of pixel coordinates in the precomputed spectrum $\bm{\lambda}_x$\\
    $N_{\rm sparse}$ & Number of non-zero pixels computed in the sparse implementation\\
    $\pm \Delta \lambda_{\mathrm{buffer}}$ & Buffer exceeding the red and blue limits of the data spectrum\\
    $P_{\rm rom}$ & The prominence threshold of spectral lines to include in cloning \\
    $v\sin{i}$ & Rotational broadening for stellar inclination $i$ and equatorial velocity $v$\\
    $RV$ & Radial velocity of the star\\
    $R$ & Spectrograph resolving power $\lambda/\delta\lambda$\\
    $\mathcal{L}$ & The loss scalar, usually the sum of the squares of the residuals\\
    \hline
    \multicolumn{2}{c}{Operators}\\
    \hline
    $\resample \big[ \mathsf{F(\bm{\lambda}_x)} \big]$ & The resample operator, takes in a flux spectrum $\mathsf{F}$ evaluated at $\bm{\lambda}_x$ coordinates and returns the mean flux within the pixel boundaries of coordinate $\bm{\lambda}_z$\\
    $*$& The convolution operator\\
    $\odot$& \emph{Hadamard product}, an elementwise product of two same-length vectors\\
    \enddata
\end{deluxetable}

\end{document}

